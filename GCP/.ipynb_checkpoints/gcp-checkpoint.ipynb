{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze | grep tensorflow==2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache-beam[gcp]==2.16.0\n",
      "  Downloading apache_beam-2.16.0-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.1,>=0.3.0\n",
      "  Downloading dill-0.3.0.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 50.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml<4.0.0,>=3.12\n",
      "  Downloading PyYAML-3.13.tar.gz (270 kB)\n",
      "\u001b[K     |████████████████████████████████| 270 kB 50.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httplib2<=0.12.0,>=0.8\n",
      "  Downloading httplib2-0.12.0.tar.gz (218 kB)\n",
      "\u001b[K     |████████████████████████████████| 218 kB 43.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fastavro<0.22,>=0.21.4\n",
      "  Downloading fastavro-0.21.24-cp37-cp37m-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (3.10.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (2020.1)\n",
      "Requirement already satisfied: avro-python3<2.0.0,>=1.8.1; python_version >= \"3.0\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.9.2.1)\n",
      "Collecting mock<3.0.0,>=1.0.1\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.4.1)\n",
      "Requirement already satisfied: grpcio<2,>=1.12.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.30.0)\n",
      "Requirement already satisfied: oauth2client<4,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (3.0.0)\n",
      "Requirement already satisfied: future<1.0.0,>=0.16.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (0.18.2)\n",
      "Requirement already satisfied: protobuf<4,>=3.5.0.post1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (3.12.3)\n",
      "Collecting pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\"\n",
      "  Downloading pyarrow-0.14.1-cp37-cp37m-manylinux2010_x86_64.whl (58.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 58.1 MB 5.7 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.7)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (2.5.8)\n",
      "Collecting google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"\n",
      "  Downloading google_cloud_datastore-1.7.4-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-core<2,>=0.28.1; extra == \"gcp\" in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]==2.16.0) (1.3.0)\n",
      "Collecting google-cloud-bigquery<1.18.0,>=1.6.0; extra == \"gcp\"\n",
      "  Downloading google_cloud_bigquery-1.17.1-py2.py3-none-any.whl (142 kB)\n",
      "\u001b[K     |████████████████████████████████| 142 kB 39.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cachetools<4,>=3.1.0; extra == \"gcp\"\n",
      "  Downloading cachetools-3.1.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"\n",
      "  Downloading google_cloud_bigtable-1.0.0-py2.py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 64.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-pubsub<1.1.0,>=0.39.0; extra == \"gcp\"\n",
      "  Downloading google_cloud_pubsub-1.0.2-py2.py3-none-any.whl (118 kB)\n",
      "\u001b[K     |████████████████████████████████| 118 kB 59.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"\n",
      "  Downloading google-apitools-0.5.28.tar.gz (172 kB)\n",
      "\u001b[K     |████████████████████████████████| 172 kB 54.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3,>=2.8.0->apache-beam[gcp]==2.16.0) (1.15.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /opt/conda/lib/python3.7/site-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]==2.16.0) (5.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /opt/conda/lib/python3.7/site-packages (from pydot<2,>=1.2.0->apache-beam[gcp]==2.16.0) (2.4.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (4.6)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<4,>=2.0.1->apache-beam[gcp]==2.16.0) (0.4.8)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf<4,>=3.5.0.post1->apache-beam[gcp]==2.16.0) (47.3.1.post20200616)\n",
      "Requirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from pyarrow<0.15.0,>=0.11.1; python_version >= \"3.0\" or platform_system != \"Windows\"->apache-beam[gcp]==2.16.0) (1.19.0)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (0.6.2)\n",
      "Requirement already satisfied: requests>=2.7.0 in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2.24.0)\n",
      "Requirement already satisfied: google-api-core[grpc]<2.0.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.21.0)\n",
      "Collecting google-resumable-media<0.5.0dev,>=0.3.1\n",
      "  Downloading google_resumable_media-0.4.1-py2.py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<1.1.0,>=0.31.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (0.12.3)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (0.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (1.25.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp]==2.16.0) (3.0.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.6.0->google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.51.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]<2.0.0dev,>=1.6.0->google-cloud-datastore<1.8.0,>=1.7.1; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.18.0)\n",
      "Requirement already satisfied: monotonic>=0.1 in /opt/conda/lib/python3.7/site-packages (from fasteners>=0.14->google-apitools<0.5.29,>=0.5.28; extra == \"gcp\"->apache-beam[gcp]==2.16.0) (1.5)\n",
      "Building wheels for collected packages: dill, pyyaml, httplib2, google-apitools\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.0-py3-none-any.whl size=77511 sha256=82dad04274632c6897a68f00b03441efef3999b461e0c552c80ec4a46ad8481d\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/6a/3c/26/1fcc712c80b81fe1859f2dda4415f180fe9ef3ebe9f5e202e4\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-3.13-cp37-cp37m-linux_x86_64.whl size=43086 sha256=55c05d982a373346e0b019bb7bbfb4e9d766fd77fefbcb253be845047a728e2a\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/95/cd/14/899edaa9cdb9a65aa7224539f6e0ad488e9a7b202bb48f6ae6\n",
      "  Building wheel for httplib2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for httplib2: filename=httplib2-0.12.0-py3-none-any.whl size=93464 sha256=e48aecb135b9d9064f56f3a05f40023634fd389181c79c560ea8e046fefc0c86\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/0d/e7/b6/0dd30343ceca921cfbd91f355041bd9c69e0f40b49f25b7b8a\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.28-py3-none-any.whl size=130110 sha256=09eb030b5ab44b15ca725ddf555003fc3a8403a94a3502585d571f5214bc74cb\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/34/3b/69/ecd8e6ae89d9d71102a58962c29faa7a9467ba45f99f205920\n",
      "Successfully built dill pyyaml httplib2 google-apitools\n",
      "\u001b[31mERROR: tfx 0.22.0 has requirement apache-beam[gcp]<3,>=2.21, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.22.0 has requirement pyarrow<0.17,>=0.16, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx-bsl 0.22.1 has requirement apache-beam[gcp]<3,>=2.20, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx-bsl 0.22.1 has requirement pyarrow<0.17,>=0.16.0, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-transform 0.22.0 has requirement apache-beam[gcp]<3,>=2.20, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.22.2 has requirement apache-beam[gcp]<3,>=2.20, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.22.2 has requirement pyarrow<0.17,>=0.16, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.22.2 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.5.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.22.2 has requirement apache-beam[gcp]<3,>=2.22, but you'll have apache-beam 2.16.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.22.2 has requirement joblib<0.15,>=0.12, but you'll have joblib 0.15.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.22.2 has requirement pyarrow<0.17,>=0.16, but you'll have pyarrow 0.14.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-cloud-storage 1.29.0 has requirement google-resumable-media<0.6dev,>=0.5.0, but you'll have google-resumable-media 0.4.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: dill, pyyaml, httplib2, fastavro, mock, pyarrow, google-cloud-datastore, google-resumable-media, google-cloud-bigquery, cachetools, google-cloud-bigtable, google-cloud-pubsub, google-apitools, apache-beam\n",
      "\u001b[33m  WARNING: The script fastavro is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script gen_client is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed apache-beam-2.16.0 cachetools-3.1.1 dill-0.3.0 fastavro-0.21.24 google-apitools-0.5.28 google-cloud-bigquery-1.17.1 google-cloud-bigtable-1.0.0 google-cloud-datastore-1.7.4 google-cloud-pubsub-1.0.2 google-resumable-media-0.4.1 httplib2-0.12.0 mock-2.0.0 pyarrow-0.14.1 pyyaml-3.13\n",
      "Requirement already satisfied: httplib2==0.12.0 in ./.local/lib/python3.7/site-packages (0.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user apache-beam[gcp]==2.16.0 \n",
    "!pip install --user httplib2==0.12.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-dlenv\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT = 'iotsensor-276409'    # CHANGE THIS\n",
    "BUCKET = 'iotsensor-276409' # REPLACE WITH YOUR BUCKET NAME. Use a regional bucket in the region you selected.\n",
    "REGION = 'us-central1' # Choose an available region for Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '2.1' \n",
    "\n",
    "## ensure we're using python3 env\n",
    "os.environ['CLOUDSDK_PYTHON'] = 'python3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n",
      "Updated property [ml_engine/local_python].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION\n",
    "\n",
    "## ensure we predict locally with our current Python environment\n",
    "gcloud config set ml_engine/local_python `which python`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query(phase, EVERY_N):\n",
    "  if EVERY_N == None:\n",
    "    EVERY_N = 4 #use full dataset\n",
    "    \n",
    "  if phase == 'train':\n",
    "  #select and pre-process fields\n",
    "      base_query = \"\"\"\n",
    "SELECT * FROM [iotsensor-276409:shrip.Shrip_ML] WHERE\n",
    "  RAND() < 0.8\n",
    "  \"\"\"\n",
    "  #add subsampling criteria by modding with hashkey\n",
    "      return base_query\n",
    "  \n",
    "  elif phase == 'valid':\n",
    "    base_query = \"\"\"\n",
    "SELECT * FROM [iotsensor-276409:shrip.Shrip_ML] WHERE\n",
    "  RAND() < 0.02\n",
    "  \"\"\"\n",
    "  #add subsampling criteria by modding with hashkey\n",
    "    return base_query\n",
    "\n",
    "  elif phase == 'test':\n",
    "    base_query = \"\"\"\n",
    "SELECT * FROM [iotsensor-276409:shrip.Shrip_ML] WHERE\n",
    "  RAND() < 0.2\n",
    "  \"\"\"\n",
    "  #add subsampling criteria by modding with hashkey\n",
    "    return base_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LUX</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Moisture</th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>e</th>\n",
       "      <th>y</th>\n",
       "      <th>nony</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.000000</td>\n",
       "      <td>1005.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>18489.970149</td>\n",
       "      <td>27.344279</td>\n",
       "      <td>38.382090</td>\n",
       "      <td>546.644776</td>\n",
       "      <td>9.697625</td>\n",
       "      <td>4.118435</td>\n",
       "      <td>0.383821</td>\n",
       "      <td>63.167505</td>\n",
       "      <td>729.091194</td>\n",
       "      <td>-10005.382405</td>\n",
       "      <td>0.38209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8327.609364</td>\n",
       "      <td>6.289883</td>\n",
       "      <td>10.459716</td>\n",
       "      <td>284.423759</td>\n",
       "      <td>0.541257</td>\n",
       "      <td>0.943601</td>\n",
       "      <td>0.104597</td>\n",
       "      <td>23.557544</td>\n",
       "      <td>186.446575</td>\n",
       "      <td>8535.436324</td>\n",
       "      <td>0.48614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4000.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>8.294050</td>\n",
       "      <td>2.558755</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>13.269222</td>\n",
       "      <td>238.510264</td>\n",
       "      <td>-27631.322811</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11610.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>9.359622</td>\n",
       "      <td>3.339849</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>43.979616</td>\n",
       "      <td>599.687988</td>\n",
       "      <td>-17320.269417</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18276.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>550.000000</td>\n",
       "      <td>9.813344</td>\n",
       "      <td>4.214420</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>66.062673</td>\n",
       "      <td>722.541629</td>\n",
       "      <td>-9418.645635</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25687.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>794.000000</td>\n",
       "      <td>10.153740</td>\n",
       "      <td>4.892790</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>83.587834</td>\n",
       "      <td>866.935231</td>\n",
       "      <td>-2212.685723</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32977.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>10.403566</td>\n",
       "      <td>5.719570</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>102.820156</td>\n",
       "      <td>1174.533860</td>\n",
       "      <td>3568.106955</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                LUX  Temperature     Humidity     Moisture            a  \\\n",
       "count   1005.000000  1005.000000  1005.000000  1005.000000  1005.000000   \n",
       "mean   18489.970149    27.344279    38.382090   546.644776     9.697625   \n",
       "std     8327.609364     6.289883    10.459716   284.423759     0.541257   \n",
       "min     4000.000000    17.000000    21.000000    50.000000     8.294050   \n",
       "25%    11610.000000    22.000000    29.000000   294.000000     9.359622   \n",
       "50%    18276.000000    27.000000    39.000000   550.000000     9.813344   \n",
       "75%    25687.000000    33.000000    47.000000   794.000000    10.153740   \n",
       "max    32977.000000    38.000000    58.000000  1100.000000    10.403566   \n",
       "\n",
       "                 b            c            e            y          nony  \\\n",
       "count  1005.000000  1005.000000  1005.000000  1005.000000   1005.000000   \n",
       "mean      4.118435     0.383821    63.167505   729.091194 -10005.382405   \n",
       "std       0.943601     0.104597    23.557544   186.446575   8535.436324   \n",
       "min       2.558755     0.210000    13.269222   238.510264 -27631.322811   \n",
       "25%       3.339849     0.290000    43.979616   599.687988 -17320.269417   \n",
       "50%       4.214420     0.390000    66.062673   722.541629  -9418.645635   \n",
       "75%       4.892790     0.470000    83.587834   866.935231  -2212.685723   \n",
       "max       5.719570     0.580000   102.820156  1174.533860   3568.106955   \n",
       "\n",
       "            final  \n",
       "count  1005.00000  \n",
       "mean      0.38209  \n",
       "std       0.48614  \n",
       "min       0.00000  \n",
       "25%       0.00000  \n",
       "50%       0.00000  \n",
       "75%       1.00000  \n",
       "max       1.00000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
    "#     which each row is represented as a python dictionary\n",
    "# Returns:\n",
    "#   -rowstring: a comma separated string representation of the record with dayofweek\n",
    "#     converted from int to string (e.g. 3 --> Tue)\n",
    "####\n",
    "def to_csv(rowdict):\n",
    "  CSV_COLUMNS = 'LUX,Temperature,Humidity,Moisture,a,b,c,e,y,nony,final'.split(',')\n",
    "  rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
    "  return rowstring\n",
    "\n",
    "\n",
    "####\n",
    "# Arguments:\n",
    "#   -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
    "#     Larger values will yield smaller sample\n",
    "#   -RUNNER: 'DirectRunner' or 'DataflowRunner'. Specfy to run the pipeline\n",
    "#     locally or on Google Cloud respectively. \n",
    "# Side-effects:\n",
    "#   -Creates and executes dataflow pipeline. \n",
    "#     See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
    "####\n",
    "def preprocess(EVERY_N, RUNNER):\n",
    "  job_name = 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "  print('Launching Dataflow job {} ... hang on'.format(job_name))\n",
    "  OUTPUT_DIR = 'gs://{0}/shrip/preproc/'.format(BUCKET)\n",
    "\n",
    "  #dictionary of pipeline options\n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
    "    'project': PROJECT,\n",
    "    'runner': RUNNER,\n",
    "    'num_workers' : 4,\n",
    "    'max_num_workers' : 5\n",
    "  }\n",
    "  #instantiate PipelineOptions object using options dictionary\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  #instantantiate Pipeline object using PipelineOptions\n",
    "  with beam.Pipeline(options=opts) as p:\n",
    "      for phase in ['train', 'valid']:\n",
    "        query = create_query(phase, EVERY_N) \n",
    "        outfile = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase))\n",
    "        (\n",
    "          p | 'read_{}'.format(phase) >> beam.io.Read(beam.io.BigQuerySource(query=query))\n",
    "            | 'tocsv_{}'.format(phase) >> beam.Map(to_csv)\n",
    "            | 'write_{}'.format(phase) >> beam.io.Write(beam.io.WriteToText(outfile))\n",
    "        )\n",
    "  print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-200803-081300 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset iotsensor-276409:temp_dataset_e6bc3b8bfea44360a6888080f9eeb822 does not exist so we will create it as temporary with location=US\n",
      "WARNING:apache_beam.io.gcp.bigquery_tools:Dataset iotsensor-276409:temp_dataset_bb98ae9bcc8445b2b26f55694fc889f7 does not exist so we will create it as temporary with location=US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(10, 'DirectRunner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://iotsensor-276409/shrip/preproc/train.csv-00000-of-00002\n",
      "gs://iotsensor-276409/shrip/preproc/train.csv-00001-of-00002\n",
      "gs://iotsensor-276409/shrip/preproc/valid.csv-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://$BUCKET/shrip/preproc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if gsutil ls | grep -q gs://$BUCKET/shrip/preproc/; then\n",
    "  gsutil -m rm -rf gs://$BUCKET/shrip/preproc/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching Dataflow job preprocess-taxifeatures-200803-174542 ... hang on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.7 interpreter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "preprocess(10, 'DataflowRunner') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13310,20,34,1006,9.49627091138916,3.01029995663981,0.34,97.1637983883894,481.785344599776,-24924.0182871429,0\n",
      "23227,20,34,718,10.0530706740756,3.01029995663981,0.34,78.3857944036106,725.074193574004,-14396.0450239844,0\n",
      "19026,21,34,765,9.8535617437664,3.33984878303764,0.34,81.6265928757481,655.993498505886,-16195.215332107,0\n",
      "19951,21,34,64,9.90103454637506,3.33984878303764,0.34,15.7617499584421,1062.43622861327,3103.76642289904,1\n",
      "27009,22,34,382,10.2039254227766,3.31132995230379,0.34,52.1805453326313,917.169876693429,-3951.47235366662,1\n",
      "15686,23,34,254,9.66052387377261,3.46184495013578,0.34,39.9503733568546,860.115348346824,-1217.6330624735,1\n",
      "6209,24,34,901,8.73375513136489,3.61235994796777,0.34,90.5955548856321,350.774255334111,-21996.9053094396,0\n",
      "32088,27,34,669,10.3762374074499,4.29409129247696,0.34,74.9185659309488,817.399689488597,-12424.8921516596,0\n",
      "12971,28,34,945,9.47047137534067,4.21441993929574,0.34,93.3824058812068,491.376981930749,-22816.3697931334,0\n",
      "14220,28,34,434,9.56240470335723,4.21441993929574,0.34,56.6888822835041,732.535191192663,-6196.52730853687,0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "#print first 10 lines of first shard of train.csv\n",
    "gsutil cat \"gs://$BUCKET/shrip/preproc/train.csv-00000-of-*\" | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [ -d sample ]; then\n",
    "  rm -rf sample\n",
    "fi\n",
    "mkdir sample\n",
    "gsutil cat \"gs://$BUCKET/shrip/preproc/train.csv-00000-of-*\" > sample/train.csv\n",
    "gsutil cat \"gs://$BUCKET/shrip/preproc/valid.csv-00000-of-*\" > sample/valid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT_COLUMNS = [\n",
      "    tf.feature_column.numeric_column('LUX'),\n",
      "    tf.feature_column.numeric_column('Temperature'),\n",
      "    tf.feature_column.numeric_column('Humidity'),\n",
      "    tf.feature_column.numeric_column('Moisture'),\n",
      "    tf.feature_column.numeric_column('a'),\n",
      "    tf.feature_column.numeric_column('b'),\n",
      "    tf.feature_column.numeric_column('c'),\n",
      "    tf.feature_column.numeric_column('e'),\n",
      "    tf.feature_column.numeric_column('y'),\n",
      "    tf.feature_column.numeric_column('nony')\n",
      "    ]\n",
      "\n",
      "def build_estimator(model_dir, nbuckets, hidden_units):\n",
      "    run_config = tf.estimator.RunConfig(save_checkpoints_secs = 30,\n",
      "                                        keep_checkpoint_max = 3)\n",
      "    my_feature_columns = []\n",
      "    for col in df_features.columns:\n",
      "       my_feature_columns.append(tf.feature_column.numeric_column(col))\n",
      "    estimator = tf.estimator.DNNClassifier(\n",
      "        model_dir = model_dir,\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "grep -A 20 \"INPUT_COLUMNS =\" shripfare/trainer/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/jupyter/shrip_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/home/jupyter/shrip_trained/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From /home/jupyter/shripfare/trainer/model.py:76: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/adagrad.py:106: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "2020-08-03 18:34:46.961127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200130000 Hz\n",
      "2020-08-03 18:34:46.961433: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x558202d62a60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-03 18:34:46.961472: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-08-03 18:34:46.961709: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /home/jupyter/shrip_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n",
      "INFO:tensorflow:loss = 278.65308, step = 0\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /home/jupyter/shrip_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-08-03T18:34:49Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/shrip_trained/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 1.58720s\n",
      "INFO:tensorflow:Finished evaluation at 2020-08-03-18:34:51\n",
      "INFO:tensorflow:Saving dict for global step 10: accuracy = 0.625, accuracy_baseline = 0.625, auc = 0.5, auc_precision_recall = 0.375, average_loss = 247.6709, global_step = 10, label/mean = 0.375, loss = 247.6709, precision = 0.0, prediction/mean = 6.067594e-07, recall = 0.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 10: /home/jupyter/shrip_trained/model.ckpt-10\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Layer dnn is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures EXCLUDED from export because they cannot be be served via TensorFlow Serving APIs:\n",
      "INFO:tensorflow:'serving_default' : Classification input must be a single string Tensor; got {'LUX': <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float64>, 'Temperature': <tf.Tensor 'Placeholder_1:0' shape=(None,) dtype=float64>, 'Humidity': <tf.Tensor 'Placeholder_2:0' shape=(None,) dtype=float64>, 'Moisture': <tf.Tensor 'Placeholder_3:0' shape=(None,) dtype=float64>, 'a': <tf.Tensor 'Placeholder_4:0' shape=(None,) dtype=float64>, 'b': <tf.Tensor 'Placeholder_5:0' shape=(None,) dtype=float64>, 'c': <tf.Tensor 'Placeholder_6:0' shape=(None,) dtype=float64>, 'e': <tf.Tensor 'Placeholder_7:0' shape=(None,) dtype=float64>, 'y': <tf.Tensor 'Placeholder_8:0' shape=(None,) dtype=float64>, 'nony': <tf.Tensor 'Placeholder_9:0' shape=(None,) dtype=float64>}\n",
      "INFO:tensorflow:'classification' : Classification input must be a single string Tensor; got {'LUX': <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float64>, 'Temperature': <tf.Tensor 'Placeholder_1:0' shape=(None,) dtype=float64>, 'Humidity': <tf.Tensor 'Placeholder_2:0' shape=(None,) dtype=float64>, 'Moisture': <tf.Tensor 'Placeholder_3:0' shape=(None,) dtype=float64>, 'a': <tf.Tensor 'Placeholder_4:0' shape=(None,) dtype=float64>, 'b': <tf.Tensor 'Placeholder_5:0' shape=(None,) dtype=float64>, 'c': <tf.Tensor 'Placeholder_6:0' shape=(None,) dtype=float64>, 'e': <tf.Tensor 'Placeholder_7:0' shape=(None,) dtype=float64>, 'y': <tf.Tensor 'Placeholder_8:0' shape=(None,) dtype=float64>, 'nony': <tf.Tensor 'Placeholder_9:0' shape=(None,) dtype=float64>}\n",
      "INFO:tensorflow:'regression' : Regression input must be a single string Tensor; got {'LUX': <tf.Tensor 'Placeholder:0' shape=(None,) dtype=float64>, 'Temperature': <tf.Tensor 'Placeholder_1:0' shape=(None,) dtype=float64>, 'Humidity': <tf.Tensor 'Placeholder_2:0' shape=(None,) dtype=float64>, 'Moisture': <tf.Tensor 'Placeholder_3:0' shape=(None,) dtype=float64>, 'a': <tf.Tensor 'Placeholder_4:0' shape=(None,) dtype=float64>, 'b': <tf.Tensor 'Placeholder_5:0' shape=(None,) dtype=float64>, 'c': <tf.Tensor 'Placeholder_6:0' shape=(None,) dtype=float64>, 'e': <tf.Tensor 'Placeholder_7:0' shape=(None,) dtype=float64>, 'y': <tf.Tensor 'Placeholder_8:0' shape=(None,) dtype=float64>, 'nony': <tf.Tensor 'Placeholder_9:0' shape=(None,) dtype=float64>}\n",
      "WARNING:tensorflow:Export includes no default signature!\n",
      "INFO:tensorflow:Restoring parameters from /home/jupyter/shrip_trained/model.ckpt-10\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /home/jupyter/shrip_trained/export/exporter/temp-1596479691/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 278.8492.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf taxifare.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/shripfare\n",
    "python -m trainer.task \\\n",
    "  --train_data_paths=${PWD}/sample/train.csv \\\n",
    "  --eval_data_paths=${PWD}/sample/valid.csv  \\\n",
    "  --output_dir=${PWD}/shrip_trained \\\n",
    "  --train_steps=10 \\\n",
    "  --job-dir=/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1596479691\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "ls shrip_trained/export/exporter/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['Humidity'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_2:0\n",
      "    inputs['LUX'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder:0\n",
      "    inputs['Moisture'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_3:0\n",
      "    inputs['Temperature'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_1:0\n",
      "    inputs['a'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_4:0\n",
      "    inputs['b'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_5:0\n",
      "    inputs['c'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_6:0\n",
      "    inputs['e'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_7:0\n",
      "    inputs['nony'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_9:0\n",
      "    inputs['y'] tensor_info:\n",
      "        dtype: DT_DOUBLE\n",
      "        shape: (-1)\n",
      "        name: Placeholder_8:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['all_class_ids'] tensor_info:\n",
      "        dtype: DT_INT32\n",
      "        shape: (-1, 2)\n",
      "        name: head/predictions/Tile:0\n",
      "    outputs['all_classes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1, 2)\n",
      "        name: head/predictions/Tile_1:0\n",
      "    outputs['class_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/ExpandDims:0\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/str_classes:0\n",
      "    outputs['logistic'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: head/predictions/logistic:0\n",
      "    outputs['logits'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1)\n",
      "        name: dnn/logits/BiasAdd:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 2)\n",
      "        name: head/predictions/probabilities:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "model_dir=$(ls ${PWD}/shrip_trained/export/exporter | tail -1)\n",
    "saved_model_cli show --dir ${PWD}/shrip_trained/export/exporter/${model_dir} --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/test1.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/test1.json\n",
    "{\"LUX\": 4973, \"Temperature\": 28, \"Humidity\": 51, \"Moisture\": 198, \"a\": 8.51177855871474, \"b\": 4.21441993929574, \"c\": 0.51, \"e\": 33.8848331626289, \"y\": 641.828972179604, \"nony\": -1030.84349379631}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /tmp/test.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/test.json\n",
    "{\"LUX\": 21650, \"Temperature\": 22, \"Humidity\": 24, \"Moisture\": 1012, \"a\": 9.98276073343064, \"b\": 3.31132995230379, \"c\": 0.24, \"e\": 97.530843046513, \"y\": 591.293104338195, \"nony\": -24590.2846315491}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL_CLASS_IDS  ALL_CLASSES  CLASS_IDS  CLASSES  LOGISTIC  LOGITS                PROBABILITIES\n",
      "[0, 1]         ['0', '1']   [0]        ['0']    [0.0]     [-549.6578979492188]  [1.0, 0.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If the signature defined in the model is not serving_default then you must specify it via --signature-name flag, otherwise the command may fail.\n",
      "WARNING: WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-08-03 18:39:25.234133: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2200130000 Hz\n",
      "2020-08-03 18:39:25.234479: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562a16771690 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-08-03 18:39:25.234516: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-08-03 18:39:25.234930: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:236: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:236: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "model_dir=$(ls ${PWD}/shrip_trained/export/exporter)\n",
    "gcloud ai-platform local predict \\\n",
    "  --model-dir=${PWD}/shrip_trained/export/exporter/${model_dir} \\\n",
    "  --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-2-gpu.2-2.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-2-gpu.2-2:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
